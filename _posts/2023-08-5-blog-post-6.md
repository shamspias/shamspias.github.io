---
title: "Adversarial Search 101: From Minimax to AlphaZeroâ€”How A I Plans When an Opponent Fights Back"
date: 2023-08-05
permalink: /posts/2023/08/adversarial-search/
tags:
  - artificial intelligence
  - game ai
  - adversarial search
  - minimax
  - alpha-beta pruning
  - monte carlo tree search
  - beginner
math: true
---

*If youâ€™ve ever played chess, Rock-Paper-Scissors, or even â€˜â€˜Guess Which Hand?â€™â€™  
you already grasp the soul of **adversarial search**:  
make a move thatâ€™s good for you **and** bad for the other side.*

This post walks through Chapter 5 style topicsâ€”one friendly slice at a time:

1. **Games & Game Trees**  
2. **Optimal Play: the Minimax Decision**  
3. **Alphaâ€“Beta Pruning (search twice as deep, same work)**  
4. **Imperfect, Real-Time Decisions (cutoffs & evaluation)**  
5. **Stochastic Games (dice on the board)**  
6. **Partially Observable Games (poker & fog-of-war)**  
7. **State-of-the-Art Game Programs**  
8. **Alternative Approaches (RL, self-play, MCTS)**  

No scary proofsâ€”just bite-size math, pocket code, and visual metaphors.

---

## 1 Games = Trees with Turns (5 Â· 1)

| Ingredient | Meaning in code |
|------------|-----------------|
| **State**  | Snapshot of the board |
| **Player(s)** | Whose turn? â€œMAXâ€ vs â€œMINâ€ |
| **Actions** | Legal moves from the state |
| **Terminal ?** | Win / lose / draw flag |
| **Utility** | +1, 0, â€“1 (or a score) |

Draw arrows for moves â†’ voilÃ , a **game tree**.  
Leaf utilities ripple *upward* to tell us which root move guarantees best outcome.

---

## 2 Optimal Decisions = **Minimax** (5 Â· 2)

> **Rule:** MAX picks the move with the **largest worst-case** utility.

$$
\text{Minimax}(s) =
\begin{cases}
\max\limits_{a}\; \text{Minimax}(\text{Succ}(s,a)) & \text{if MAX turn}\\[4pt]
\min\limits_{a}\; \text{Minimax}(\text{Succ}(s,a)) & \text{if MIN turn}\\[4pt]
\text{Utility}(s) & \text{if terminal}
\end{cases}
$$

Think of it as MAX saying â€œAssume my rival is perfect; what move still leaves me happiest?â€

---

### 2Â·1 One-Screen Demo â€“ Pure Minimax Tic-Tac-Toe

```python
def minimax(state, player):
    if terminal(state):
        return utility(state), None
    best_val = -float('inf') if player=='X' else float('inf')
    best_act = None
    for a in actions(state):
        val, _ = minimax(result(state, a), opponent(player))
        if (player=='X' and val>best_val) or (player=='O' and val<best_val):
            best_val, best_act = val, a
    return best_val, best_act
```

Worksâ€¦ but visits every node â†’ **slow** for chess-sized trees.

---

## 3 Alphaâ€“Beta Pruning (5 Â· 3)

> Skip exploring any branch that **canâ€™t possibly** change the answer.

- Keep two bounds: **Î±** = best found for MAX so far, **Î²** = best for MIN.  
- If a nodeâ€™s value proves **â‰¤ Î±** or **â‰¥ Î²**, prune!

**Win:** same answer as full minimax; often cuts search **by an order of magnitude**.

```python
def alphabeta(s, depth, Î±, Î², player):
    if depth==0 or terminal(s):
        return eval(s)
    if player=='MAX':
        v=-float('inf')
        for a in actions(s):
            v = max(v, alphabeta(result(s,a), depth-1, Î±, Î², 'MIN'))
            Î± = max(Î±, v)
            if Î² <= Î±: break          # Î²-cutoff
        return v
    else:
        v=float('inf')
        for a in actions(s):
            v = min(v, alphabeta(result(s,a), depth-1, Î±, Î², 'MAX'))
            Î² = min(Î², v)
            if Î² <= Î±: break          # Î±-cutoff
        return v
```

Add **iterative deepening** + **move ordering** â†’ search 6â€“8 plies in chess under a second on a laptop.

---

## 4 Imperfect, Real-Time Decisions (5 Â· 4)

Big games need answers **before** the hourglass empties.

1. **Depth Cut-off** â€“ search N plies, then use `eval(state)`.
2. **Evaluation Function** â€“ quick maths that estimates utility.  
   - Chess: *material + mobility + king safety*.  
   - Othello: *disc diff + corner possession*.
3. **Iterative Deepening** â€“ run depth 1, 2, 3â€¦ until time up â†’ always have a best move ready.

---

## 5 Stochastic Games (5 Â· 5)

Add dice â†’ use **Expectiminimax**:

$$
V(s) =
\begin{cases}
\max_a V(\text{Succ}(s,a)) & \text{MAX node}\\
\min_a V(\text{Succ}(s,a)) & \text{MIN node}\\
\sum_{s'} P(s'|s,a)\,V(s') & \text{Chance node}\\
\end{cases}
$$

Backgammon agents roll with this plus alpha-beta-style pruning called **Ïƒ-cuts**.

---

## 6 Partially Observable Games (5 Â· 6)

Poker, Battleship, Strategoâ€¦

- Maintain **belief states** over hidden cards.  
- Use **Monte Carlo Sampling**: simulate many deals consistent with your observations, run minimax/MCTS on each, pick the move with highest average payoff.

> **Liberatus (Poker AI)** crushed pros via **Counterfactual Regret Minimization (CFR)**â€”it learns a near-Nash strategy across **all** hidden possibilities.

---

## 7 State-of-the-Art Game Programs (5 Â· 7)

| Game      | Technique                           | Milestone |
|-----------|-------------------------------------|-----------|
| **Chess** | Alphaâ€“Beta + heuristics + table-bases | Deep Blue (1997) |
| **Go**    | **Monte Carlo Tree Search + Deep Nets** | AlphaGo (2016) |
| **Shogi / Chess 2.0** | **Self-play RL (AlphaZero)** | 2017 |
| **Poker** | CFR + self-play                       | Libratus (2017) |
| **StarCraft II** | **Policy gradient + MCTS + scripted micro** | AlphaStar (2019) |

---

## 8 Alternative Approaches (5 Â· 8)

1. **Monte Carlo Tree Search (MCTS)** â€“ build tree by sampling playouts; balance explore/exploit with **UCT**.
2. **Reinforcement Learning** â€“ learn evaluation or policy via self-play (TD-Gammon â†’ AlphaZero).
3. **Neuro-Symbolic Mixes** â€“ deep nets for pattern recognition, symbolic search for exact tactics.

---

## 9 Cheat-Sheet ðŸ§¾

```
Game tree       = Nodes alternate MAX / MIN / chance
Minimax         = Optimal play vs perfect foe
Alpha-Beta      = Same result, prunes hopeless branches
Cutoff + Eval   = Play fast with an informed guess
Expectiminimax  = Minimax + probability at chance nodes
Belief state    = Probability over hidden info
MCTS            = Build tree by random rollouts (good for Go)
AlphaZero       = Self-play RL + MCTS â†’ beats everything
```

---

## 10 Try It Yourself ðŸ§ª

1. **Write an Alpha-Beta Connect-Four**  
   Use a simple evaluation (center-column bonus). How many plies can you reach in <2 seconds?
2. **MCTS vs Minimax Tic-Tac-Toe**  
   Replace evaluation with 10 000 random rollouts per moveâ€”see who wins more.
3. **Stochastic Mini-Yahtzee**  
   Code a tiny expectiminimax; compare vs a greedy baseline.

---

### ðŸš€Final Words

Classical search finds paths in a friendly world, but **adversarial search** fights a moving target.

- **Minimax** teaches defence.  
- **Alphaâ€“Beta** teaches efficiency.  
- **MCTS & self-play RL** teach *intuition* beyond brute force.

Master these and your code will not just *solve* puzzlesâ€”it will **outfox living, plotting opponents**.

**Happy gaming, and may your branch always prune early!**