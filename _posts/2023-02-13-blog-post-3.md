---
title: "Machine Learning 101: PyTorch, TensorFlow & Decision Trees"
date: 2023-02-13
permalink: /posts/2023/02/ml-101-pytorch-tf-decision-tree/
tags:
  - machine learning
  - deep learning
  - pytorch
  - tensorflow
  - decision tree
  - beginner
  - python
math: true
---

*If you can put toys into big and small groups,  
or ask â€œIs it an animal?â€ when you play **Guess Who?**,  
then you already think like a machine learning model!*

This post answers three gentle questions, one baby step at a time:

1. **What are PyTorch and TensorFlow?**
2. **Why do we need them?**
3. **How does a Decision Tree work?**

No scary symbolsâ€”just tiny doses of math, colourful pictures, and short code you can run in a notebook.

---

## 1  Why Bother With ML *Libraries*?

Imagine you have to count all the marbles in a swimming poolâ€”**by hand**. Ouch! A calculator (or a friendly big sibling)
would help. In ML the â€œmarblesâ€ are *millions* of numbers. PyTorch and TensorFlow are the helpful siblings that:

| Hard thing                 | The library does it for you               |
|----------------------------|-------------------------------------------|
| Keep track of every number | Wrap them in **tensors**                  |
| Do giant chains of maths   | Use your computerâ€™s **GPU** (super fast)  |
| Work out derivatives       | Provide **autograd** (automatic calculus) |
| Save & reload models       | Oneâ€‘line functions like `torch.save()`    |

Result: you spend time on *ideas*, not on counting marbles.

---

## 2  Meet the Friendly Giants: PyTorch vs TensorFlow

|              | **PyTorch**                        | **TensorFlow / Keras**                             |
|--------------|------------------------------------|----------------------------------------------------|
| *Feels like* | Regular Python + NumPy             | Building blocks that snap into a *graph*           |
| Main fans    | Researchers, hobby projects        | Large companies, production apps                   |
| Debugging    | `print()` works instantly          | Needs the Keras â€œeagerâ€ switch (now on by default) |
| Fancy extras | TorchAudio, TorchVision, TorchText | TensorBoard, TFâ€‘Lite (phones), TPUs                |

> **Tiny ruleâ€‘ofâ€‘thumb:**
> *Prototyping fast?* â†’ **PyTorch**.  
> *Shipping to millions of phones?* â†’ **TensorFlow**.

### 2Â·1  Oneâ€‘Screen Demo â€“ Linear Regression in Each Library

#### PyTorch

```python
import torch, torch.nn as nn

a = torch.randn(100, 1)
b = 3 * a + 0.5 + 0.1 * torch.randn_like(a)

model = nn.Linear(1, 1)
optim = torch.optim.SGD(model.parameters(), lr=0.1)
loss_fn = nn.MSELoss()

for _ in range(300):
    optim.zero_grad()
    loss = loss_fn(model(a), b)
    loss.backward()
    optim.step()

print(model.weight.item(), model.bias.item())  # ~3.0 and ~0.5
```

#### TensorFlow / Keras

```python
import tensorflow as tf
from tensorflow.keras import layers

# Generate data
x = tf.random.normal((100, 1))
y = 3 * x + 0.5 + 0.1 * tf.random.normal((100, 1))

# Define the model using Input layer
model = tf.keras.Sequential([
    tf.keras.Input(shape=(1,)),
    layers.Dense(1)
])

model.compile(optimizer="sgd", loss="mse")
model.fit(x, y, epochs=300, verbose=0)

# Print learned weights and bias
print(model.weights[0].numpy(), model.weights[1].numpy())  # ~3.0, ~0.5
```

Same maths, same answerâ€”the libraries just do the heavy lifting.

---

## 3  Decision Trees: The â€œ20 Questionsâ€ Algorithm

### 3Â·1A Story First

Picture a basket of **fruit**: apples and oranges. You want a *robot kid* to tell them apart.

1. **Ask a yes/no question** like *â€œIs the fruit orangeâ€‘coloured?â€*
2. Put every fruit that says **yes** on the left, every **no** on the right.
3. Keep asking new questions on each pile until **every pile holds only one kind** of fruit.
4. To classify a *new* fruit, start at the top question and follow the answers down to a leaf node.

Thatâ€™s a **Decision Tree**â€”nothing fancier than a flowâ€‘chart of yes/no gates.

### 3Â·2  How Does the Tree Pick a â€œGoodâ€ Question?

It chooses the question that makes the piles **cleaner**.

We measure â€˜cleanâ€‘nessâ€™ (impurity) with a small formula called **entropy**. If a pile is half apples, half oranges, it
is *messy* (entropy â‰ˆ 1). If a pile is all apples, entropy is 0.

Hereâ€™s the math bite (skip if you like):

$$
H(S) = -\sum_{c} p_c \log_2 p_c,
$$

where $$ (p_c) $$ is the fraction of class *c* in set *S*.

**Information Gain** is the drop in entropy after a split:

$$
\text{Gain} = H(\text{parent}) - \bigl(\tfrac{|L|}{|S|}H(L) + \tfrac{|R|}{|S|}H(R)\bigr).
$$

The tree tries every question it can think of and picks the one with the **largest gain**.

*(Donâ€™t panic: Scikitâ€‘Learn computes this for you.)*

### 3Â·3  Handsâ€‘On Example With Only Four Fruits

| # | Colour score (0 = light, 1 = dark) | Diameter cm | Label |
|:-:|:----------------------------------:|:-----------:|:-----:|
| 1 |                 0                  |     3.0     |  ğŸŠ   |
| 2 |                 0                  |     3.2     |  ğŸŠ   |
| 3 |                 1                  |     3.4     |  ğŸ   |
| 4 |                 1                  |     3.6     |  ğŸ   |

Try the question **â€œColour â‰¤ 0.5?â€**

* Left pile â†’ rows 1 & 2 â†’ all oranges â†’ entropy = 0
* Right pile â†’ rows 3 & 4 â†’ all apples â†’ entropy = 0
* Parent entropy â‰ˆ 1.0 â†’ **Gain = 1.0 â€“ 0 = 1.0** (perfect!)

One question and the job is done.

### 3Â·4  Code: Grow & Draw a Tree

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

X, y = load_iris(return_X_y=True)
model = DecisionTreeClassifier(max_depth=3, criterion="entropy").fit(X, y)

plt.figure(figsize=(10, 6))
plot_tree(model,
          feature_names=["sepal len", "sepal wid", "petal len", "petal wid"],
          class_names=load_iris().target_names,
          filled=True, rounded=True);
plt.show()
```

Run it in a notebookâ€”the picture spells out every yes/no gate.

---

## 4  Trees vs Neural Nets at a Glance

|                          | **Decision Tree**          | **Neural Net (PyTorch / TF)** |
|--------------------------|----------------------------|-------------------------------|
| How it learns            | Greedy splits, no calculus | Gradient descent + autograd   |
| Needs scaling?           | **No**                     | Often **yes**                 |
| Explains itself easily?  | **Yes** (draw the tree)    | Hard (needs extra tools)      |
| Loves small tabular data | âœ”ï¸                         | âŒ (needs lots of data)        |

---

## 5  Try It Yourself ğŸ§ª

1. **Change the Fruit Basket**  
   Make up your own table with *shape*, *colour*, *weight* and run `DecisionTreeClassifier` again.
2. **PyTorch vs TensorFlow**  
   Rewrite the tiny linearâ€‘regression demo above in the *other* library.
3. **Entropy on Paper**  
   Calculate the entropy of `Colour â‰¤ 0.5` split by handâ€”see that it really is 0.

---

## 6 Cheatâ€‘Sheet ğŸ§¾

```
PyTorch          = Pythonic, researchâ€‘friendly tensor toolkit
TensorFlow/Keras = Endâ€‘toâ€‘end, productionâ€‘friendly ML factory
Decision Tree    = A flowâ€‘chart that asks yes/no questions to classify data

Entropy          = Messiness in a pile (0 = pure, 1 â‰ˆ mixed)
Info Gain        = Entropy drop after a split
Best Split       = Question with the biggest Info Gain
```

---

### ğŸš€Final Words

If you can sort fruit or play â€œ20 Questions,â€ you already grasp the soul of Decision Trees.

Frameworks like **PyTorch** and **TensorFlow** are simply powerful calculators that help your computer juggle the
numbers so you donâ€™t have to.

Master these gentle ideas and the door to the ML playground swings wide open.

**Happy learningâ€”and may your questions always split the pile just right!**

