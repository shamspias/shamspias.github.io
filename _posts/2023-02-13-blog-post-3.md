---
title: "Machine Learning 101: PyTorch, TensorFlow & Decision Trees"
date: 2023-02-13
permalink: /posts/2023/02/ml-101-pytorch-tf-decision-tree/
tags:
  - machine learning
  - deep learning
  - pytorch
  - tensorflow
  - decision tree
  - beginner
  - python
math: true
---

*If you can put toys into big and small groups,  
or ask “Is it an animal?” when you play **Guess Who?**,  
then you already think like a machine learning model!*

This post answers three gentle questions, one baby step at a time:

1. **What are PyTorch and TensorFlow?**
2. **Why do we need them?**
3. **How does a Decision Tree work?**

No scary symbols—just tiny doses of math, colourful pictures, and short code you can run in a notebook.

---

## 1  Why Bother With ML *Libraries*?

Imagine you have to count all the marbles in a swimming pool—**by hand**. Ouch! A calculator (or a friendly big sibling)
would help. In ML the “marbles” are *millions* of numbers. PyTorch and TensorFlow are the helpful siblings that:

| Hard thing                 | The library does it for you               |
|----------------------------|-------------------------------------------|
| Keep track of every number | Wrap them in **tensors**                  |
| Do giant chains of maths   | Use your computer’s **GPU** (super fast)  |
| Work out derivatives       | Provide **autograd** (automatic calculus) |
| Save & reload models       | One‑line functions like `torch.save()`    |

Result: you spend time on *ideas*, not on counting marbles.

---

## 2  Meet the Friendly Giants: PyTorch vs TensorFlow

|              | **PyTorch**                        | **TensorFlow / Keras**                             |
|--------------|------------------------------------|----------------------------------------------------|
| *Feels like* | Regular Python + NumPy             | Building blocks that snap into a *graph*           |
| Main fans    | Researchers, hobby projects        | Large companies, production apps                   |
| Debugging    | `print()` works instantly          | Needs the Keras “eager” switch (now on by default) |
| Fancy extras | TorchAudio, TorchVision, TorchText | TensorBoard, TF‑Lite (phones), TPUs                |

> **Tiny rule‑of‑thumb:**
> *Prototyping fast?* → **PyTorch**.  
> *Shipping to millions of phones?* → **TensorFlow**.

### 2·1  One‑Screen Demo – Linear Regression in Each Library

#### PyTorch

```python
import torch, torch.nn as nn

a = torch.randn(100, 1)
b = 3 * a + 0.5 + 0.1 * torch.randn_like(a)

model = nn.Linear(1, 1)
optim = torch.optim.SGD(model.parameters(), lr=0.1)
loss_fn = nn.MSELoss()

for _ in range(300):
    optim.zero_grad()
    loss = loss_fn(model(a), b)
    loss.backward()
    optim.step()

print(model.weight.item(), model.bias.item())  # ~3.0 and ~0.5
```

#### TensorFlow / Keras

```python
import tensorflow as tf
from tensorflow.keras import layers

# Generate data
x = tf.random.normal((100, 1))
y = 3 * x + 0.5 + 0.1 * tf.random.normal((100, 1))

# Define the model using Input layer
model = tf.keras.Sequential([
    tf.keras.Input(shape=(1,)),
    layers.Dense(1)
])

model.compile(optimizer="sgd", loss="mse")
model.fit(x, y, epochs=300, verbose=0)

# Print learned weights and bias
print(model.weights[0].numpy(), model.weights[1].numpy())  # ~3.0, ~0.5
```

Same maths, same answer—the libraries just do the heavy lifting.

---

## 3  Decision Trees: The “20 Questions” Algorithm

### 3·1A Story First

Picture a basket of **fruit**: apples and oranges. You want a *robot kid* to tell them apart.

1. **Ask a yes/no question** like *“Is the fruit orange‑coloured?”*
2. Put every fruit that says **yes** on the left, every **no** on the right.
3. Keep asking new questions on each pile until **every pile holds only one kind** of fruit.
4. To classify a *new* fruit, start at the top question and follow the answers down to a leaf node.

That’s a **Decision Tree**—nothing fancier than a flow‑chart of yes/no gates.

### 3·2  How Does the Tree Pick a “Good” Question?

It chooses the question that makes the piles **cleaner**.

We measure ‘clean‑ness’ (impurity) with a small formula called **entropy**. If a pile is half apples, half oranges, it
is *messy* (entropy ≈ 1). If a pile is all apples, entropy is 0.

Here’s the math bite (skip if you like):

$$
H(S) = -\sum_{c} p_c \log_2 p_c,
$$

where $$ (p_c) $$ is the fraction of class *c* in set *S*.

**Information Gain** is the drop in entropy after a split:

$$
\text{Gain} = H(\text{parent}) - \bigl(\tfrac{|L|}{|S|}H(L) + \tfrac{|R|}{|S|}H(R)\bigr).
$$

The tree tries every question it can think of and picks the one with the **largest gain**.

*(Don’t panic: Scikit‑Learn computes this for you.)*

### 3·3  Hands‑On Example With Only Four Fruits

| # | Colour score (0 = light, 1 = dark) | Diameter cm | Label |
|:-:|:----------------------------------:|:-----------:|:-----:|
| 1 |                 0                  |     3.0     |  🍊   |
| 2 |                 0                  |     3.2     |  🍊   |
| 3 |                 1                  |     3.4     |  🍎   |
| 4 |                 1                  |     3.6     |  🍎   |

Try the question **“Colour ≤ 0.5?”**

* Left pile → rows 1 & 2 → all oranges → entropy = 0
* Right pile → rows 3 & 4 → all apples → entropy = 0
* Parent entropy ≈ 1.0 → **Gain = 1.0 – 0 = 1.0** (perfect!)

One question and the job is done.

### 3·4  Code: Grow & Draw a Tree

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

X, y = load_iris(return_X_y=True)
model = DecisionTreeClassifier(max_depth=3, criterion="entropy").fit(X, y)

plt.figure(figsize=(10, 6))
plot_tree(model,
          feature_names=["sepal len", "sepal wid", "petal len", "petal wid"],
          class_names=load_iris().target_names,
          filled=True, rounded=True);
plt.show()
```

Run it in a notebook—the picture spells out every yes/no gate.

---

## 4  Trees vs Neural Nets at a Glance

|                          | **Decision Tree**          | **Neural Net (PyTorch / TF)** |
|--------------------------|----------------------------|-------------------------------|
| How it learns            | Greedy splits, no calculus | Gradient descent + autograd   |
| Needs scaling?           | **No**                     | Often **yes**                 |
| Explains itself easily?  | **Yes** (draw the tree)    | Hard (needs extra tools)      |
| Loves small tabular data | ✔️                         | ❌ (needs lots of data)        |

---

## 5  Try It Yourself 🧪

1. **Change the Fruit Basket**  
   Make up your own table with *shape*, *colour*, *weight* and run `DecisionTreeClassifier` again.
2. **PyTorch vs TensorFlow**  
   Rewrite the tiny linear‑regression demo above in the *other* library.
3. **Entropy on Paper**  
   Calculate the entropy of `Colour ≤ 0.5` split by hand—see that it really is 0.

---

## 6 Cheat‑Sheet 🧾

```
PyTorch          = Pythonic, research‑friendly tensor toolkit
TensorFlow/Keras = End‑to‑end, production‑friendly ML factory
Decision Tree    = A flow‑chart that asks yes/no questions to classify data

Entropy          = Messiness in a pile (0 = pure, 1 ≈ mixed)
Info Gain        = Entropy drop after a split
Best Split       = Question with the biggest Info Gain
```

---

### 🚀Final Words

If you can sort fruit or play “20 Questions,” you already grasp the soul of Decision Trees.

Frameworks like **PyTorch** and **TensorFlow** are simply powerful calculators that help your computer juggle the
numbers so you don’t have to.

Master these gentle ideas and the door to the ML playground swings wide open.

**Happy learning—and may your questions always split the pile just right!**

