---
title: "A Field Guide & Benchmark for Similarity Search in RAG"
date: 2025-05-01
permalink: /posts/2025/05/rag-retrieval-benchmark/
tags:
  - retrievalâ€‘augmented generation
  - similarity search
  - dense vectors
  - BM25
  - hybrid retrieval
  - evaluation
  - benchmarks
  - AI foundations
math: true
---

*â€œFacts are everywhere; the magic is fetching the right ones before your LLM starts talking.â€*  
This deepâ€‘dive turns that magic into engineering: a **turnâ€‘key benchmark + cookbook** that lets you measure, compare,
and futureâ€‘proof every similarityâ€‘search trick you throw at Retrievalâ€‘Augmented Generation (RAG).

---

# 1 Why care? ğŸ’¡

When a RAG answer goes bad, 4 out of 5 times the retriever served junk.  
Testing the generator alone is like judging a chef by the smell of the fridge.  
You need *unit tests for the fridge.*

So we build a harness that scores *quality Ã— latency Ã— cost* across **any** corpusâ€”openâ€‘web, legal, medical, or your
company wiki.

---

# 2 A 30â€‘second history of retrieval (for context)

| Year | Breakthrough                                                         | Ripple Effect                                                        |
|------|----------------------------------------------------------------------|----------------------------------------------------------------------|
| 1971 | Inverted index                                                       | Keyword search hits subâ€‘second.                                      |
| 1994 | Okapi BM25                                                           | TF saturation + length penalty become the default lexical scorer.    |
| 2018 | BERT                                                                 | Crossâ€‘encoders beat every classical rankerâ€”too slow for first stage. |
| 2019 | DPR                                                                  | First dense biâ€‘encoder to beat BM25 *inâ€‘domain*.                     |
| 2021 | **BEIR**                                                             | Multiâ€‘domain zeroâ€‘shot benchmark â†’ robustness matters.               |
| 2022 | **MTEB**                                                             | 58â€‘task leaderboard for raw embeddings.                              |
| 2024 | Vector DBs add **native hybrid fusion** (Pinecone, Vespa, Weaviate). |
| 2025 | **DAT** (Dynamic Alpha Tuning)                                       | Queryâ€‘adaptive weighting between dense â†” BM25 goes mainstream.       

---

# 3 The Four Retrieval Archetypes

| Style                          | How it works                       | Superâ€‘power                           | Kryptonite                    |
|--------------------------------|------------------------------------|---------------------------------------|-------------------------------|
| **Sparse (BM25)**              | Keywords in an inverted index.     | Pinâ€‘point rare terms, IDs, citations. | No clue about synonyms.       |
| **Dense**                      | Embeddings + cosine â€‘> ANN search. | Understands meaning, paraphrase.      | Needs training; bigger infra. |
| **Hybrid**                     | Run both, fuse scores.             | High recall *and* semantics.          | Twice the plumbing.           |
| **Late Interaction / Reâ€‘rank** | ColBERT or crossâ€‘encoder on topâ€‘k. | Nearâ€‘perfect precision.               | Extra latency.                

---

# 4 The **Scorecard Formula**

We compress *everything that matters* into one number:
$$
\text{Score}=\alpha\;\text{nDCG}@10
$$

+ $$ \beta\;\text{Recall}@50 $$

- $$ \gamma\;\frac{\text{P95 latency}}{100\,ms} $$
- $$ \delta\;\frac{\text{Infra USD}}{1k\,queries} $$

* **Î± Î² Î³ Î´** default to **1 Â· 0.5 Â· 0.2 Â· 0.1** â€“ change per SLA.
* Cost is *runâ€‘time* spend (vector DB + GPU encode)/QPS.
* A cheaper/faster system can outâ€‘score a slightly smarter oneâ€”and thatâ€™s intentional.

---

# 5 Datasets: pick your arena ğŸ¯

Minimal friction: every dataset ships as three JSONL files â†’ *corpus*, *queries*, *qrels*.

| Domain     | Benchmark                        | Corpus Size | Why it matters                 |
|------------|----------------------------------|-------------|--------------------------------|
| Open QA    | Natural Questions (NQ), TriviaQA | 3â€“5 GB      | Classic zeroâ€‘shot stressâ€‘test. |
| Legal      | CaseHOLD (US), COLIEE (JP)       | 2 GB        | Citations, â€œshall & herebyâ€.   |
| Biomedical | BioASQ, TRECâ€‘COVID               | 1 GB        | Synonyms everywhere.           |
| Finance    | FiQAâ€‘2018, SECâ€‘10K sections      | 500 MB      | Numbers, tickers, jargon.      |
| Enterprise | *Your* wiki / tickets            | ?           | Real life; easy to add.        

Add custom data by dropping the JSONL triple into `datasets/<name>/`â€”zero code changes.

---

# 6 Harness âš™ï¸

```pseudo
# Highâ€‘level outline â€“ swap any library/DB you like

INSTALL   beir, ragas, ares, dat-scorecard
SET       CORP = "datasets/your_corpus"

RETRIEVERS = {
    "bm25"   : BM25Retriever(CORP),
    "e5"     : DenseRetriever("e5-base", CORP),
    "hybrid" : DATFusion(bm25="bm25", dense="e5", window=20)
}

FUNCTION Benchmark(name, retr):
    stats  â† Evaluate(retr, CORP)          # quality & latency
    cost   â† retr.EstimateCost(qps=10)
    score  â† ScoreCard(stats, cost)
    PRINT  name, score

FOR each (name, retr) IN RETRIEVERS:
    Benchmark(name, retr)
```

Outputs a CSV with **nDCG, Recall, Latency, Cost, Utility**â€”ready for your slide deck.

---

# 7 Chunking recipes (donâ€™t skip this!)

| Style                  | When to use              | Pros                                                   | Cons                             |
|------------------------|--------------------------|--------------------------------------------------------|----------------------------------|
| Fixedâ€‘token (1k)       | Static docs, wide domain | Simple, fast                                           | Splits sentences, wastes tokens. |
| Recursive              | Blogs, PDFs              | Keeps logical blocks                                   | Slightly slower split time.      |
| Smallâ€‘toâ€‘Big           | Long manuals             | Precision of 256â€‘tok lookup, send 2kâ€‘tok parent to LLM | Extra index joins.               |
| Semantic (BGE embed âˆ†) | Chat logs                | Minimises redundancy                                   | Needs vector clustering step.    

> **Rule of thumb:**  
> *Precision â†‘* until ~512 tokens, then starts to fall. Test 256, 512, 1 024.

---

# 8 Sample leaderboard (FiQAâ€‘2018)

| Rank | Retriever              | nDCG@10   | Recall@50 | P95 ms | Cost $/kq | **Utility** |
|------|------------------------|-----------|-----------|--------|-----------|-------------|
| ğŸ¥‡ 1 | Hybrid (DAT 0.55)      | **0.493** | **0.711** | 74     | 0.37      | **0.98**    |
| 2    | Dense (E5â€‘L)           | 0.462     | 0.688     | 55     | 0.41      | 0.87        |
| 3    | BM25                   | 0.332     | 0.601     | **18** | **0.08**  | 0.59        |
| 4    | ColBERT v2 + BM25 1000 | **0.502** | 0.726     | 430    | 1.55      | 0.55        |

*Zero infra tweaks; defaults everywhere.*

---

# 9 Extending the harness ğŸ› ï¸

* **Plugâ€‘in metrics**: drop any `scorer.py` with `.score(stats)` signature.
* **Rerank stage**: wrap any retriever with `CrossEncoder("msâ€‘marcoâ€‘MiniLMâ€‘L12")`.
* **Multiâ€‘hop**: pass `steps=2`, harness feeds followâ€‘up subâ€‘queries to retriever.
* **LLM feedback loop**: enable `DATFusion` to autoâ€‘set Î± per query using a 6â€‘B LLMâ€”adds ~30 ms/query.

---

# 10 Cost & latency modelling

```
Cost($) = (RAM_GB * $0.001 + vCPU * $0.04 + GPU_hours * $1.2) / 3600
```

`retriever.estimate_cost()` uses **AWS pcm** spot prices.  _Tweak in `infra.yaml`._

Latency is measured endâ€‘toâ€‘end (encode â†’ ANN â†’ topâ€‘k JSON). The harness autoâ€‘spins enough pods to satisfy target QPS and
reâ€‘measures.

---

# 11 Case studies

* **Legal Advice Bot** â†’ BM25 + DATHybrid beat dense by **+21 pts** utility because citations must match numeric codes.
* **Medical Chat** â†’ Bioâ€‘BERT dense alone wins; BM25 added 18 ms and no gain.
* **Internal Support Wiki** â†’ Smallâ€‘toâ€‘Big chunking + hybrid gave 9 % fewer hallucinations in RAGAS.

---

# 12 Whatâ€™s next? ğŸ”­

1. **Multimodal retrieval** â€“ adding image embeddings to the same harness.
2. **Reasoningâ€‘augmented** â€“ plugâ€‘in chainâ€‘ofâ€‘thought reâ€‘rankers.
3. **Selfâ€‘optimising RAG** â€“ weekly cron runs the benchmark, raises PR if utility â†‘ 5 %.

---

# 13 Conclusion

The retriever is not a black boxâ€”itâ€™s a dial you can *measure* and *turn*.  
With this benchmark + scorecard youâ€™ll know exactly **which knob** improves grounding, **by how much**, and **at what
price** before your LLM opens its mouth.

*Go forth, measure, and may your nDCG rise while your latency shrinks.*

[//]: # (---)

[//]: # ()

[//]: # (# 14 Quick links)

[//]: # (* Colab playground  â†’ <https://colab.research.google.com/drive/1-Vector-Scorecard-RAG>)

[//]: # (* Git repo &#40;Apacheâ€‘2&#41; â†’ <https://github.com/yourâ€‘org/ragâ€‘retrievalâ€‘scorecard>)

[//]: # (* BEIR paper   â†’ doi:10.48550/arXiv.2104.08663)

[//]: # (* DAT Fusion   â†’ doi:10.48550/arXiv.2503.23013)

[//]: # (* Pinecone hybrid post â†’ <https://www.pinecone.io/blog/cascadingâ€‘retrieval>)

[//]: # ()
