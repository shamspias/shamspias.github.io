---
title: "Intelligent Agents & Search: A Baby-Steps Tour from â€˜What is Rational?â€™ to â€˜How do we find the goal?â€™"
date: 2023-04-26
permalink: /posts/2023/04/ai-agents-search/
tags:
  - artificial intelligence
  - intelligent agents
  - search algorithms
  - heuristic
  - beginner
  - python
math: true
---

*If youâ€™ve ever followed Google Maps or played hide-and-seek,  
you already know the heart of **agents** and **search**.*

This friendly post tackles two classic A.I. chapters, one toddler step at a time:

1. **Chapter 2 â€“ Intelligent Agents**  
   (What *is* an agent? What counts as smart?)
2. **Chapter 3 â€“ Solving Problems by Searching**  
   (How do agents *actually* reach the goal?)

No heavy jargonâ€”just pocket-sized math, cartoons-in-words, and run-it-now code.

---

## 1 Intelligent Agents

### 1Â·1 Agents & Environments
> **Agent =** *thing that perceives â†’ acts.*  
> **Environment =** the playground it lives in.

| Example agent | Sensors (percepts) | Actuators (actions) | Environment |
|---------------|--------------------|---------------------|-------------|
| Roomba        | bump sensor, IR    | wheels, vacuum      | Living room |
| Chess AI      | board state        | move generator      | Chess game  |

**Loop**: sense â†’ think â†’ act, forever.

---

### 1Â·2 Good Behavior = Rationality
An agent is **rational** if, *given what it can sense*, it does whatever **maximizes its expected score**.

$$
a^* = \arg\!\max\nolimits_{a} \mathbb{E}[\text{Performance} \mid \text{percepts}, a]
$$

Think â€œbest guessâ€, *not* perfect prophecyâ€”your GPS canâ€™t foresee roadworks, yet still picks the shortest-known route.

---

### 1Â·3 The Nature of Environments

| Property         | Meaning                         | Mnemonic |
|------------------|---------------------------------|----------|
| **Observable?**  | Can the agent see *everything*? | â€œEyesâ€   |
| **Deterministic?**| Same action = same outcome?     | â€œLuckâ€   |
| **Episodic?**    | Does today ignore yesterday?    | â€œTV showâ€|
| **Static?**      | Does the world change on its own?| â€œFreezeâ€ |
| **Discrete?**    | Finite chunks vs real numbers?  | â€œBoxesâ€  |

A **fully-observable, deterministic, static, discrete** world (e.g. chess) is easy-mode. A **partially-observable, stochastic, dynamic, continuous** world (e.g. driving) is nightmare-mode.

---

### 1Â·4 Inside the Agent: Four Flavors

1. **Simple Reflex** â€“ *If dirt â†’ Suck.*  
2. **Model-Based Reflex** â€“ remembers unseen rooms.
3. **Goal-Based** â€“ plans toward *finish line*.  
4. **Utility-Based** â€“ maximizes happiness points.

---

### 1Â·5 One-Screen Demo â€“ Reflex Vacuum

```python
def reflex_vacuum(percept):
    room, state = percept              # e.g. ("A", "Dirty")
    rules = {("A", "Dirty"): "Suck",
             ("B", "Dirty"): "Suck",
             ("A", "Clean"): "Right",
             ("B", "Clean"): "Left"}
    return rules[(room, state)]
```

Drop it in a loop that feeds back new perceptsâ€”voilÃ , Chapter 2 in 12 lines.

---

## 2 Solving Problems by Searching

### 2Â·1 Problem-Solving Agents
Now the agent has **goal = find path**.  
Input: problem description.  
Output: sequence of actions reaching_goal.

---

### 2Â·2 Classic Toy Problems

| Puzzle                | Start â†’ Goal                                  |
|-----------------------|-----------------------------------------------|
| **8-Puzzle**          | Scramble tiles â†’ arrange 1-2-3 â€¦ 8-blank       |
| **Route Finding**     | City A â†’ City B (shortest kilometres)         |
| **Missionaries & Cannibals** | Everyone crosses river safely          |

Pick one â†’ plug into a generic search engine.

---

### 2Â·3 Search Trees & Graphs

* **Node** = world-state + path-cost so far.  
* **Tree** because we *branch* on every action.  
* **Frontier** = â€œopen listâ€ of leaves to explore next.

---

### 2Â·4 Uninformed (Blind) Search

| Strategy | Fringe order | Optimal? | Memory | When use it |
|----------|--------------|----------|--------|-------------|
| **BFS**  | shallowest   | âœ”        | huge   | Few steps, costs equal |
| **DFS**  | deepest      | âœ–        | tiny   | Really deep, any solution okay |
| **Uniform-Cost** | lowest path-cost | âœ” | huge | Varying step costs |

---

### 2Â·5 Informed (Heuristic) Search

A **heuristic** \( h(n) \) estimates distance-to-goal.  
Good \( h \) = cheap to compute, *never* overestimates (â†’ admissible).

| Strategy | Uses \( h(n) \) | Optimal? |
|----------|-----------------|----------|
| **Greedy Best-First** | as priority | âœ– (fast, risky) |
| **A\***               | \( f(n)=g+h \) | âœ” (if \( h \) admissible) |

> **Rule of thumb:**  
> *Maze?* â†’ **A\*** with Manhattan distance.  
> *GPS?* â†’ **A\*** with straight-line kilometres.

---

### 2Â·6 Code: Tiny A\* on a Grid

```python
from heapq import heappush, heappop

def astar(start, goal, walls, width, height):
    def h(p):           # Manhattan distance
        return abs(p[0]-goal[0]) + abs(p[1]-goal[1])

    frontier = []
    heappush(frontier, (h(start), 0, start, []))  # (f, g, node, path)
    explored = set()

    while frontier:
        f, g, node, path = heappop(frontier)
        if node == goal:
            return path + [node]

        if node in explored: 
            continue
        explored.add(node)

        for dx, dy in [(1,0),(-1,0),(0,1),(0,-1)]:
            nxt = (node[0]+dx, node[1]+dy)
            if 0<=nxt[0]<width and 0<=nxt[1]<height and nxt not in walls:
                heappush(frontier, (g+1+h(nxt), g+1, nxt, path+[node]))
```

Drop a rectangle of walls, hit `astar((0,0),(9,9), walls, 10,10)`â€”watch the path pop out.

---

## 3 Cheat-Sheet ðŸ§¾

```
Agent          = Sensor + Actuator + Program
Rationality    = Best expected outcome, given whatâ€™s known
Environment    = {observable?, deterministic?, episodic?, static?, discrete?}
Reflex         = Rule table
Goal-Based     = Needs search
Utility-Based  = Needs numbers

Search Tree    = Nodes of (state, cost)
BFS/DFS/UCS    = No heuristics
Heuristic h(n) = Guess of distance-to-goal
Greedy         = Smallest h(n)
A*             = Smallest g(n)+h(n)  (optimal if h never overestimates)
```

---

## 4 Try It Yourself ðŸ§ª

1. **Vacuum Variations**  
   Add a third room **C**â€”watch the rules explode.  
   Now code a *model-based* agent instead.
2. **Write Your Own Heuristic**  
   For the 8-puzzle, implement `h = number_of_misplaced_tiles`. Then upgrade to *Manhattan distance*.
3. **Blind vs Heuristic Race**  
   Time BFS vs A\* on a 20Ã—20 maze. Count nodes expandedâ€”see the power of a good guess.

---

### ðŸš€Final Words
Whether itâ€™s a Roomba dodging socks or a path-planner steering Mars rovers, an **agent** must *decide* and often must *search*.  
Keep the loop simpleâ€”**perceive â†’ plan â†’ act**â€”and arm it with a smart heuristic.  
Soon your code wonâ€™t just tidy the living room; itâ€™ll conquer any maze you care to draw.

**Happy coding, and may your heuristic always point the way!**