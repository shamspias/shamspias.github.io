---
title: "Retrieval Metrics Demystified: From BM25 Baselines to EM@5 & Answer F1"
date: 2024-08-15
permalink: /posts/2024/08/retrieval-metrics/
tags:
  - information retrieval
  - RAG
  - evaluation
  - BM25
  - dense retrieval
  - question answering
  - metrics
  - beginner
math: true
---

*â€œIf a fact falls in a database and nobody retrieves it, does it make a sound?â€*  
Retrievalâ€‘Augmented Generation (RAG) lives or dies on that first hopâ€”**can the system put the right snippets in front of the language model?**  
In this post we peel back the buzzwords (*BM25*, *EM@5*, *F1*) and show how to turn them into levers you can actually pull.

---

## 1 Why bother measuring retrieval separately?

Endâ€‘toâ€‘end metrics (BLEU, ROUGE, human ratings) blur two questions together:

1. *Did I pull the right passages?*
2. *Did the generator use them well?*

Untangling the knot matters. If you log a 5â€‘point jump in answer F1, you want to know **where** the jump came fromâ€”better retrieval, a smarter prompt, or a lucky seed? The retrieval metrics below give you that Xâ€‘ray.

---

## 2 BM25â€”the keyword workhorse

Before transformers, there was the **inverted index**: a glorified phonebook where every word points to the documents it lives in. BM25 (â€œBest Match 25â€) is the score those phonebooks still use today:

$$
\operatorname{BM25}(q,d)=\sum_{t\in q} \text{IDF}(t)\;\frac{f(t,d)(k_1+1)}{f(t,d)+k_1\bigl(1-b+b\tfrac{|d|}{\overline{|d|}}\bigr)}\;
$$

- *f(t,d)*         = term frequency of *t* in document *d*  
- *\|d\|*             = token length of *d*  
- *IDF(t)*        = inverse document frequency  
- Default hyperâ€‘params: $$ (k_1\approx1.2) $$ , $$ (b\approx0.75) $$

> **Mental model:** BM25 is a tugâ€‘ofâ€‘war between *how often* a query word shows up and *how common* that word is across the whole corpus.

Why keep it around?

* **Speed** â€“ microseconds per query on millions of docs.  
* **Transparency** â€“ devs can still debug with Ctrlâ€‘F.  
* **Baseline gravity** â€“ if you canâ€™t beat BM25, somethingâ€™s off.

---

## 3 EM@kâ€”Exact Match at *k*

Imagine playing *Whereâ€™s Waldo?* but youâ€™re allowed to search the first *k* pages instead of the whole book. **EM@k** asks: *â€œDoes any of my topâ€‘k passages contain the gold answer string **exactly**?â€*

Algorithm for a question set of size \(N\):

1. Retrieve topâ€‘*k* passages per question.  
2. Mark **hit = 1** if at least one passage contains the gold answer, otherwise 0.  
3. $$ (\displaystyle \text{EM@k}=\frac{\sum_{i=1}^{N} \text{hit}_i}{N}) $$.

*Why the fuss over exact match?*  
Because partial overlaps (â€œ2008 financial crashâ€ vs. â€œthe 2008 recessionâ€) are slippery to grade at retrieval time. EM@k stays dumb on purposeâ€”either the string shows up or it doesnâ€™t.

> **Ruleâ€‘ofâ€‘thumb:**  
> *EM@5 â‰¥ 80 %* â†’ retrieval is likely *not* your bottleneck.  
> *EM@5 â‰¤ 60 %* â†’ focus on the retriever before promptâ€‘tuning.

---

## 4 Answerâ€‘level F1â€”did the generator actually use the context?

Once your passages hit the jackpot, the generator still has to *say* the answer. For extractive QA the goâ€‘to metric is tokenâ€‘level **F1**:

$$
\text{F1}=\frac{2\,\text{precision}\times\text{recall}}{\text{precision}+\text{recall}}
$$

| Component | Definition |
|-----------|------------|
| **Precision** | Tokens in the model answer âˆ© tokens in the gold answer Ã· tokens in the model answer |
| **Recall**    | Tokens in the model answer âˆ© tokens in the gold answer Ã· tokens in the gold answer |

F1 forgives small wording tweaksâ€”*â€œBarack Obamaâ€* vs. *â€œObamaâ€*â€”in a way EM cannot.

---

## 5 From BM25 to Dense Retrieval & Reranking

| Stage | Model | What changes                                                           | Why you win |
|-------|-------|------------------------------------------------------------------------|-------------|
| **Dualâ€‘encoder** | *Dense Passage Retriever* | Index contains 768â€‘D vectors, not word positions                       | Captures synonyms (â€œterminateâ€ â‰ˆ â€œcancelâ€) |
| **Crossâ€‘encoder** | MiniLM, MonoT5â€¦ | Reâ€‘score $$ ([\text{CLS}] q\;[SEP]\;d) $$ with full token interactions | Sharp ordering; filters noise |

A typical contract QA study logged:

* BM25 â†’ **61 % EM@5**  
* DPR + Crossâ€‘encoder â†’ **79 % EM@5**

Same corpus, same questionsâ€”just a richer notion of â€œrelevanceâ€.

---

## 6 Other retrieval diagnostics youâ€™ll meet in the wild

| Metric | What it asks | Best whenâ€¦ |
|--------|--------------|-----------|
| **Recall@k** | *Any* gold passage in topâ€‘*k*? | Gold labels are full passages, not spans |
| **MRR** (Mean Reciprocal Rank) | How early is the **first** correct hit? | You care about position 1 above all |
| **MAP** (Mean Avg. Precision) | How well are **all** relevant docs ranked? | Multiple correct passages per query |
| **nDCG@k** | Same as MAP but with graded (0â€“3) relevance | Web search, ad ranking |

---

## 7 Handsâ€‘on: computing EM@5 in Python

```python
from collections import Counter
from typing import List

def em_at_k(retrieved: List[List[str]], gold: List[str], k: int = 5) -> float:
    """retrieved[i] is the ranked list for question i; gold[i] the gold answer string"""
    hits = sum(any(gold[i] in doc for doc in retrieved[i][:k]) for i in range(len(gold)))
    return hits / len(gold)
```

> **Pro tip:** preâ€‘lowercase and strip punctuation on both sides to avoid false misses.

---

## 8 Cheatâ€‘sheet ğŸ§¾

```
BM25         â€“ bagâ€‘ofâ€‘words baseline; fast, transparent
EM@k         â€“ % questions whose answer text appears in topâ€‘k passages
Answer F1    â€“ token overlap between generated and gold answer
Dense Retr.  â€“ dualâ€‘encoder embeddings; higher recall than BM25
Crossâ€‘encode â€“ reranks with full attention; boosts topâ€‘1 precision
```

---

## 9 Try it yourself ğŸ§ª

1. **FAQ Retriever Bakeâ€‘Off**  
   Index your company FAQ with BM25 *and* DPR; measure EM@5 on a 50â€‘question test set. Which wins?
2. **Promptâ€‘Effect Audit**  
   Freeze retrieval; vary only the generation prompt. How much does answer F1 move? Log your findings in a twoâ€‘column table.
3. **Metric Mixing Board**  
   Build a dashboard that shows EM@1, EM@5, Recall@20, and answer F1 side by side for each experiment run.

---

## 10 Final words

Like good coffee, a RAG system is only as strong as its first extraction. Nail the retrieval metrics and the language model can do what it does bestâ€”explain, summarise, and synthesise without hallucinating. Happy hunting, and may your EM curves trend ever upward!

---

## 11 Live Demo in Colab

I had packed the full retrieval-metrics pipelineâ€”including BM25 retrieval, EM@k scoring, token-level F1, and EM-curve plottingâ€”into a runnable Google Colab notebook. Click below to open, run, and experiment:

[Open the â€œRetrieval Metrics Demystifiedâ€ Colab notebook](https://colab.research.google.com/drive/1IzCYnxtvM1fPPrCVW4SMAyo7aYdFWSeX?usp=sharing)

Feel free to:

- Fork and modify the corpus or QA set  
- Tune BM25 hyper-parameters (`k1`, `b`)  
- Swap in a dense retriever or reranker  
- Plot EM@k curves on your own data  

Comments and pull-requests on the notebook are very welcomeâ€”let me know what you build!